{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Caminho de origem dos arquivos JSON brutos no Volume\n",
    "SOURCE_PATH = \"/Volumes/spotify_analytics/landing/raw_data/spotify_data_raw_*.json\"\n",
    "\n",
    "# Configuração destino\n",
    "CATALOGO_DESTINO = \"spotify_analytics\"\n",
    "SCHEMA_DESTINO = \"bronze\"\n",
    "TABELA_DESTINO = \"tb_bronze_search\"\n",
    "\n",
    "nome_tabela_destino = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_DESTINO}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração\n",
    "\n",
    "Define:\n",
    "- **SOURCE_PATH**: Localização dos arquivos JSON no Unity Catalog Volume (landing layer)\n",
    "- **Destino**: Tabela Bronze onde serão armazenados os dados brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de Arquivos JSON como Texto\n",
    "\n",
    "Estratégia: Lê JSON completo como STRING para preservar estrutura original.\n",
    "- Usa `format(\"text\")` com `wholetext=true` para capturar arquivo inteiro\n",
    "- Adiciona metadata do arquivo fonte usando `_metadata.file_path` (Unity Catalog)\n",
    "- Adiciona timestamp de ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê arquivos JSON como texto bruto\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .format(\"text\")\n",
    "    .option(\"wholetext\", \"true\")\n",
    "    .load(SOURCE_PATH)\n",
    ")\n",
    "\n",
    "# Adiciona metadata (Unity Catalog usa _metadata.file_path)\n",
    "df_bronze = (\n",
    "    df_raw\n",
    "    .withColumnRenamed(\"value\", \"raw_json\")\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"ingestion_date\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "print(f\"Total de arquivos lidos: {df_bronze.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga na Tabela Bronze\n",
    "\n",
    "Escreve dados na tabela Delta usando modo **APPEND**:\n",
    "- Permite cargas incrementais\n",
    "- Mantém histórico de todas as ingestões\n",
    "- `mergeSchema=true` para flexibilidade de schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_bronze.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(nome_tabela_destino)\n",
    ")\n",
    "\n",
    "print(f\"✅ Dados carregados com sucesso em {nome_tabela_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação Final\n",
    "\n",
    "Confirma quantidade de registros na tabela Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_registros = spark.table(nome_tabela_destino).count()\n",
    "print(f\"Total de registros em {nome_tabela_destino}: {total_registros}\")\n",
    "\n",
    "# Mostra últimas ingestões\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT ingestion_date, COUNT(*) as qtd_registros\n",
    "    FROM {nome_tabela_destino}\n",
    "    GROUP BY ingestion_date\n",
    "    ORDER BY ingestion_date DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
