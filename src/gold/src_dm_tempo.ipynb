{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "TABELA_ORIGEM = \"spotify_analytics.silver.tb_tracks\"\n",
    "TABELA_DESTINO = \"spotify_analytics.gold.dm_tempo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração\n",
    "\n",
    "Define origem (Silver tb_tracks) e destino (Gold dm_tempo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação Silver → Gold (Dimensão Tempo)\n",
    "\n",
    "Cria dimensão temporal com todas as datas de lançamento presentes no dataset.\n",
    "\n",
    "**Processo:**\n",
    "1. Extrai datas únicas de release_date\n",
    "2. Para cada data, deriva:\n",
    "   - cd_tempo: Formato YYYYMMDD (ex: 20240515)\n",
    "   - Componentes: ano, mês, dia\n",
    "   - Nome do mês por extenso\n",
    "   - Trimestre (Q1, Q2, Q3, Q4)\n",
    "   - Semestre (S1, S2)\n",
    "   - Década (2020s, 2010s, etc.)\n",
    "\n",
    "**Decisão Arquitetural**: Dimensão tempo pré-populada vs on-demand:\n",
    "- **Abordagem escolhida**: On-demand (apenas datas existentes nos dados)\n",
    "- **Alternativa**: Pre-populate (gerar 50 anos de datas antecipadamente)\n",
    "- **Justificativa**: Dataset limitado, não precisamos de datas futuras\n",
    "- **Trade-off**: Se precisar analisar períodos sem lançamentos, não terão linhas na dimensão\n",
    "\n",
    "Faz MERGE em dm_tempo (Gold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = spark.read.table(TABELA_ORIGEM)\n",
    "\n",
    "# Extrai datas únicas\n",
    "df_dates = (\n",
    "    df_silver\n",
    "    .select(\"release_date\")\n",
    "    .filter(F.col(\"release_date\").isNotNull())\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Deriva atributos temporais\n",
    "df_gold = df_dates.select(\n",
    "    # Surrogate key: YYYYMMDD\n",
    "    F.date_format(F.col(\"release_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"cd_tempo\"),\n",
    "    \n",
    "    F.col(\"release_date\").alias(\"dt_completa\"),\n",
    "    \n",
    "    # Componentes numéricos\n",
    "    F.year(\"release_date\").alias(\"nu_ano\"),\n",
    "    F.month(\"release_date\").cast(\"smallint\").alias(\"nu_mes\"),\n",
    "    F.dayofmonth(\"release_date\").cast(\"smallint\").alias(\"nu_dia\"),\n",
    "    \n",
    "    # Nome do mês\n",
    "    F.date_format(\"release_date\", \"MMMM\").alias(\"nm_mes\"),\n",
    "    \n",
    "    # Trimestre\n",
    "    F.concat(F.lit(\"Q\"), F.quarter(\"release_date\")).alias(\"ds_trimestre\"),\n",
    "    \n",
    "    # Semestre\n",
    "    F.when(F.month(\"release_date\") <= 6, \"S1\").otherwise(\"S2\").alias(\"ds_semestre\"),\n",
    "    \n",
    "    # Década\n",
    "    F.concat(\n",
    "        (F.floor(F.year(\"release_date\") / 10) * 10).cast(\"string\"),\n",
    "        F.lit(\"s\")\n",
    "    ).alias(\"ds_decada\")\n",
    ")\n",
    "\n",
    "print(f\"Total de datas únicas: {df_gold.count()}\")\n",
    "df_gold.orderBy(F.desc(\"cd_tempo\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE em dm_tempo\n",
    "\n",
    "Match por cd_tempo (surrogate key YYYYMMDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable.forName(spark, TABELA_DESTINO).alias(\"t\").merge(\n",
    "    df_gold.alias(\"s\"),\n",
    "    \"t.cd_tempo = s.cd_tempo\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n",
    "\n",
    "print(f\"✅ Dimensão Tempo carregada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição por década\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT ds_decada, COUNT(*) as qtd_datas\n",
    "    FROM {TABELA_DESTINO}\n",
    "    GROUP BY ds_decada\n",
    "    ORDER BY ds_decada DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Distribuição por trimestre (últimos 2 anos)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT nu_ano, ds_trimestre, COUNT(*) as qtd_datas\n",
    "    FROM {TABELA_DESTINO}\n",
    "    WHERE nu_ano >= (SELECT MAX(nu_ano) - 1 FROM {TABELA_DESTINO})\n",
    "    GROUP BY nu_ano, ds_trimestre\n",
    "    ORDER BY nu_ano DESC, ds_trimestre\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
