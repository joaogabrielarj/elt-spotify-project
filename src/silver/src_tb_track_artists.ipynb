{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, BooleanType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "CATALOGO_ORIGEM = \"spotify_analytics\"\n",
    "SCHEMA_ORIGEM = \"bronze\"\n",
    "TABELA_ORIGEM = \"tb_bronze_search\"\n",
    "\n",
    "CATALOGO_DESTINO = \"spotify_analytics\"\n",
    "SCHEMA_DESTINO = \"silver\"\n",
    "TABELA_DESTINO = \"tb_track_artists\"\n",
    "TABELA_INVALIDOS_DESTINO = \"tb_track_artists_invalidos\"\n",
    "\n",
    "nome_tabela_origem = f\"{CATALOGO_ORIGEM}.{SCHEMA_ORIGEM}.{TABELA_ORIGEM}\"\n",
    "nome_tabela_destino = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_DESTINO}\"\n",
    "nome_tabela_invalidos = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_INVALIDOS_DESTINO}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração\n",
    "\n",
    "Define origem (Bronze), destino (Silver) e tabela de auditoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Explícito para JSON (com Artists)\n",
    "\n",
    "Além da estrutura de tracks, inclui array de artistas:\n",
    "- `tracks.items[].artists[]`: Array de artistas por track\n",
    "- Cada artista tem: id, name\n",
    "\n",
    "Estratégia de Explode Duplo:\n",
    "1. Primeiro explode: tracks.items → uma linha por track\n",
    "2. Segundo explode: artists → uma linha por combinação track-artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Schema incluindo artists (array direto na raiz)\nartist_schema = StructType([\n    StructField(\"id\", StringType(), True),\n    StructField(\"name\", StringType(), True)\n])\n\nspotify_schema = StructType([\n    StructField(\"items\", ArrayType(StructType([\n        StructField(\"id\", StringType(), True),\n        StructField(\"artists\", ArrayType(artist_schema), True)\n    ])), True)\n])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura Incremental + Double Explode\n",
    "\n",
    "Lê última carga do Bronze e aplica transformações:\n",
    "1. Parseia JSON\n",
    "2. **Explode 1**: tracks.items\n",
    "3. **Explode 2**: artists array\n",
    "4. Remove duplicatas por (track_id, artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lê última carga do Bronze\ndf_bronze = spark.read.table(nome_tabela_origem)\n\nmax_dt_ingestao = (\n    df_bronze\n    .agg(F.max(F.col(\"ingestion_date\")).alias(\"max_ts\"))\n    .first()[\"max_ts\"]\n)\n\n# Parseia JSON\ndf_parsed = (\n    df_bronze\n    .filter(F.col(\"ingestion_date\") == F.lit(max_dt_ingestao))\n    .withColumn(\"parsed_data\", F.from_json(F.col(\"raw_json\"), spotify_schema))\n    .select(\n        F.explode(F.col(\"parsed_data.items\")).alias(\"track\"),\n        \"ingestion_date\",\n        \"source_file\"\n    )\n)\n\n# Explode artistas (segundo explode)\ndf_exploded_artists = (\n    df_parsed\n    .select(\n        F.col(\"track.id\").alias(\"track_id\"),\n        F.explode(F.col(\"track.artists\")).alias(\"artist\"),\n        \"ingestion_date\"\n    )\n)\n\n# Extrai campos e limpa\ndf_limpo = (\n    df_exploded_artists\n    .select(\n        F.col(\"track_id\"),\n        F.col(\"artist.id\").alias(\"artist_id\"),\n        F.trim(F.col(\"artist.name\")).alias(\"artist_name\"),\n        F.col(\"ingestion_date\").alias(\"dt_ingestion\"),\n        F.lit(\"spotify_api_search\").alias(\"dc_origem\")\n    )\n    .dropDuplicates([\"track_id\", \"artist_id\"])\n)\n\nprint(f\"Registros de track-artists: {df_limpo.count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação de Qualidade\n",
    "\n",
    "Adiciona flags de validação:\n",
    "- **flag_track_id_valido**: track_id IS NOT NULL\n",
    "- **flag_artist_id_valido**: artist_id IS NOT NULL\n",
    "- **flag_artist_name_valido**: artist_name IS NOT NULL\n",
    "- **flag_qualidade**: \"OK\" se todas flags TRUE, senão \"ERRO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validacao = (\n",
    "    df_limpo\n",
    "    .withColumn(\"flag_track_id_valido\", F.col(\"track_id\").isNotNull())\n",
    "    .withColumn(\"flag_artist_id_valido\", F.col(\"artist_id\").isNotNull())\n",
    "    .withColumn(\"flag_artist_name_valido\", F.col(\"artist_name\").isNotNull())\n",
    "    .withColumn(\"flag_qualidade\",\n",
    "        F.when(\n",
    "            F.col(\"flag_track_id_valido\") &\n",
    "            F.col(\"flag_artist_id_valido\") &\n",
    "            F.col(\"flag_artist_name_valido\"),\n",
    "            F.lit(\"OK\")\n",
    "        ).otherwise(F.lit(\"ERRO\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_validos = df_validacao.filter(F.col(\"flag_qualidade\") == \"OK\")\n",
    "df_invalidos = df_validacao.filter(F.col(\"flag_qualidade\") == \"ERRO\")\n",
    "\n",
    "# Remove flags dos registros válidos\n",
    "df_silver = df_validos.select(\n",
    "    \"track_id\", \"artist_id\", \"artist_name\", \"dt_ingestion\", \"dc_origem\"\n",
    ")\n",
    "\n",
    "print(f\"Registros válidos: {df_validos.count()}\")\n",
    "print(f\"Registros inválidos: {df_invalidos.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE de Registros Válidos\n",
    "\n",
    "Faz MERGE (UPSERT) em tb_track_artists:\n",
    "- Match na chave composta (track_id, artist_id)\n",
    "- **MATCHED**: Atualiza\n",
    "- **NOT MATCHED**: Insere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forName(spark, nome_tabela_destino)\n",
    "\n",
    "delta_table.alias(\"destino\").merge(\n",
    "    df_silver.alias(\"origem\"),\n",
    "    \"destino.track_id = origem.track_id AND destino.artist_id = origem.artist_id\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n",
    "\n",
    "print(f\"✅ Tabela {nome_tabela_destino} atualizada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERWRITE de Registros Inválidos\n",
    "\n",
    "Sobrescreve tabela de auditoria com registros rejeitados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_tabela_invalidos)\n",
    "\n",
    "print(f\"✅ Tabela {nome_tabela_invalidos} atualizada para auditoria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação Final\n",
    "\n",
    "Exibe estatísticas da tabela bridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de combinações track-artist\n",
    "total = spark.table(nome_tabela_destino).count()\n",
    "print(f\"Total de track-artists na Silver: {total}\")\n",
    "\n",
    "# Top 5 artistas com mais tracks\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT artist_name, COUNT(DISTINCT track_id) as qtd_tracks\n",
    "    FROM {nome_tabela_destino}\n",
    "    GROUP BY artist_name\n",
    "    ORDER BY qtd_tracks DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}