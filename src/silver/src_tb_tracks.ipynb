{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, BooleanType, LongType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "CATALOGO_ORIGEM = \"spotify_analytics\"\n",
    "SCHEMA_ORIGEM = \"bronze\"\n",
    "TABELA_ORIGEM = \"tb_bronze_search\"\n",
    "\n",
    "CATALOGO_DESTINO = \"spotify_analytics\"\n",
    "SCHEMA_DESTINO = \"silver\"\n",
    "TABELA_DESTINO = \"tb_tracks\"\n",
    "TABELA_INVALIDOS_DESTINO = \"tb_tracks_invalidos\"\n",
    "\n",
    "nome_tabela_origem = f\"{CATALOGO_ORIGEM}.{SCHEMA_ORIGEM}.{TABELA_ORIGEM}\"\n",
    "nome_tabela_destino = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_DESTINO}\"\n",
    "nome_tabela_invalidos = f\"{CATALOGO_DESTINO}.{SCHEMA_DESTINO}.{TABELA_INVALIDOS_DESTINO}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração\n",
    "\n",
    "Define origem (Bronze), destino (Silver) e tabela de auditoria (inválidos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Explícito para JSON\n",
    "\n",
    "Define estrutura esperada do JSON da API Spotify Search:\n",
    "- `tracks.items[]`: Array de tracks\n",
    "- Cada track tem: id, name, album, popularity, duration_ms, explicit\n",
    "- Album contém: name, album_type, release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Schema do JSON customizado (array de tracks direto na raiz)\nspotify_schema = StructType([\n    StructField(\"items\", ArrayType(StructType([\n        StructField(\"id\", StringType(), True),\n        StructField(\"name\", StringType(), True),\n        StructField(\"popularity\", IntegerType(), True),\n        StructField(\"duration_ms\", LongType(), True),\n        StructField(\"explicit\", BooleanType(), True),\n        StructField(\"album\", StructType([\n            StructField(\"name\", StringType(), True),\n            StructField(\"album_type\", StringType(), True),\n            StructField(\"release_date\", StringType(), True)\n        ]), True)\n    ])), True)\n])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF: Normalização de Data de Lançamento\n",
    "\n",
    "A API retorna datas em formatos variáveis:\n",
    "- \"2024\" → normaliza para \"2024-01-01\"\n",
    "- \"2024-05\" → normaliza para \"2024-05-01\"\n",
    "- \"2024-05-15\" → mantém\n",
    "\n",
    "Permite conversão para DATE de forma consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def normalize_release_date(date_string):\n",
    "    \"\"\"Normaliza formatos variáveis de data para yyyy-MM-dd\"\"\"\n",
    "    if date_string is None:\n",
    "        return None\n",
    "    \n",
    "    parts = date_string.split(\"-\")\n",
    "    \n",
    "    if len(parts) == 1:  # Apenas ano\n",
    "        return f\"{parts[0]}-01-01\"\n",
    "    elif len(parts) == 2:  # Ano-mês\n",
    "        return f\"{parts[0]}-{parts[1]}-01\"\n",
    "    else:  # Data completa\n",
    "        return date_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura Incremental do Bronze\n",
    "\n",
    "Lê apenas a última carga (max ingestion_date):\n",
    "- Parseia JSON usando schema explícito\n",
    "- Explode array de tracks\n",
    "- Normaliza data de lançamento\n",
    "- Remove duplicatas por track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lê última carga do Bronze\ndf_bronze = spark.read.table(nome_tabela_origem)\n\nmax_dt_ingestao = (\n    df_bronze\n    .agg(F.max(F.col(\"ingestion_date\")).alias(\"max_ts\"))\n    .first()[\"max_ts\"]\n)\n\n# Parseia JSON e explode tracks\ndf_parsed = (\n    df_bronze\n    .filter(F.col(\"ingestion_date\") == F.lit(max_dt_ingestao))\n    .withColumn(\"parsed_data\", F.from_json(F.col(\"raw_json\"), spotify_schema))\n    .select(\n        F.explode(F.col(\"parsed_data.items\")).alias(\"track\"),\n        \"ingestion_date\",\n        \"source_file\"\n    )\n)\n\n# Extrai e limpa campos\ndf_limpo = (\n    df_parsed\n    .select(\n        F.col(\"track.id\").alias(\"track_id\"),\n        F.trim(F.col(\"track.name\")).alias(\"track_name\"),\n        F.trim(F.col(\"track.album.name\")).alias(\"album_name\"),\n        F.col(\"track.album.album_type\").alias(\"album_type\"),\n        F.to_date(\n            normalize_release_date(F.col(\"track.album.release_date\")), \n            \"yyyy-MM-dd\"\n        ).alias(\"release_date\"),\n        F.col(\"track.popularity\").alias(\"popularity\"),\n        F.col(\"track.duration_ms\").alias(\"duration_ms\"),\n        F.col(\"track.explicit\").alias(\"explicit\"),\n        F.col(\"ingestion_date\").alias(\"dt_ingestion\"),\n        F.lit(\"spotify_api_search\").alias(\"dc_origem\")\n    )\n    .withColumn(\"release_year\", F.year(F.col(\"release_date\")))\n    .dropDuplicates([\"track_id\"])\n)\n\nprint(f\"Registros lidos e limpos: {df_limpo.count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação de Qualidade\n",
    "\n",
    "Adiciona flags de validação:\n",
    "- **flag_id_valido**: track_id IS NOT NULL\n",
    "- **flag_nome_valido**: track_name IS NOT NULL AND length > 1\n",
    "- **flag_duracao_valida**: duration_ms > 0\n",
    "- **flag_qualidade**: \"OK\" se todas flags TRUE, senão \"ERRO\"\n",
    "\n",
    "Split em df_validos e df_invalidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validacao = (\n",
    "    df_limpo\n",
    "    .withColumn(\"flag_id_valido\", F.col(\"track_id\").isNotNull())\n",
    "    .withColumn(\"flag_nome_valido\", \n",
    "        F.col(\"track_name\").isNotNull() & (F.length(F.col(\"track_name\")) > 1)\n",
    "    )\n",
    "    .withColumn(\"flag_duracao_valida\", \n",
    "        F.col(\"duration_ms\").isNotNull() & (F.col(\"duration_ms\") > 0)\n",
    "    )\n",
    "    .withColumn(\"flag_qualidade\",\n",
    "        F.when(\n",
    "            F.col(\"flag_id_valido\") &\n",
    "            F.col(\"flag_nome_valido\") &\n",
    "            F.col(\"flag_duracao_valida\"),\n",
    "            F.lit(\"OK\")\n",
    "        ).otherwise(F.lit(\"ERRO\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_validos = df_validacao.filter(F.col(\"flag_qualidade\") == \"OK\")\n",
    "df_invalidos = df_validacao.filter(F.col(\"flag_qualidade\") == \"ERRO\")\n",
    "\n",
    "# Remove flags dos registros válidos\n",
    "df_silver = df_validos.select(\n",
    "    \"track_id\", \"track_name\", \"album_name\", \"album_type\",\n",
    "    \"release_date\", \"release_year\", \"popularity\", \n",
    "    \"duration_ms\", \"explicit\", \"dt_ingestion\", \"dc_origem\"\n",
    ")\n",
    "\n",
    "print(f\"Registros válidos: {df_validos.count()}\")\n",
    "print(f\"Registros inválidos: {df_invalidos.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE de Registros Válidos\n",
    "\n",
    "Faz MERGE (UPSERT) em tb_tracks:\n",
    "- **MATCHED**: Atualiza registro existente\n",
    "- **NOT MATCHED**: Insere novo registro\n",
    "\n",
    "Operação idempotente (pode ser re-executada sem gerar duplicatas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forName(spark, nome_tabela_destino)\n",
    "\n",
    "delta_table.alias(\"destino\").merge(\n",
    "    df_silver.alias(\"origem\"),\n",
    "    \"destino.track_id = origem.track_id\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n",
    "\n",
    "print(f\"✅ Tabela {nome_tabela_destino} atualizada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERWRITE de Registros Inválidos\n",
    "\n",
    "Sobrescreve tb_tracks_invalidos com registros rejeitados da última carga.\n",
    "Contém flags de validação para rastreabilidade de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalidos.write.format(\"delta\").mode(\"overwrite\").saveAsTable(nome_tabela_invalidos)\n",
    "\n",
    "print(f\"✅ Tabela {nome_tabela_invalidos} atualizada para auditoria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação Final\n",
    "\n",
    "Exibe métricas da tabela Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de tracks\n",
    "total = spark.table(nome_tabela_destino).count()\n",
    "print(f\"Total de tracks na Silver: {total}\")\n",
    "\n",
    "# Top 5 tracks mais populares\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT track_name, album_name, popularity\n",
    "    FROM {nome_tabela_destino}\n",
    "    ORDER BY popularity DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}